{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea3_2(4).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"mo95-O7JSJgV","colab_type":"text"},"cell_type":"markdown","source":["<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/MARCA-Color.jpg\" title=\"Title text\" width=\"50%\" height=\"50%\" />\n","\n","\n","<hr style=\"height:2px;border:none\"/>\n","<h1 align='center'> INF-395/477 Redes Neuronales Artificiales II-2018 </h1>\n","\n","<H3 align='center'> Tarea 3 - Redes Recurrentes y Autoencoders </H3>\n","<hr style=\"height:2px;border:none\"/>\n","\n","\n","\n","**Temas**  \n","* Diseño e implementación de Redes Neuronales Recurrentes (RNN).\n","* Regularización en Redes Recurrentes.\n","* Autoencoders tradicionales y sus aplicaciones\n","\n","** Formalidades **  \n","* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n","* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n","* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n","* Fecha de entrega y discusión: 21 de Diciembre (sin posibilidad de extensiones)\n","* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<margarita.bugueno.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<cvalle@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea3-INF395-II-2018] \n","\n","<hr style=\"height:2px;border:none\"/>\n","\n","La tarea se divide en dos secciones:\n","\n","[1.](#primero) RNN sobre texto    \n","[2.](#segundo) Autoencoder en MNIST  \n","\n","\n","### **Nota Importante:**  \n","Para esta actividad **si es que no se cuenta con GPU** se recomienda utilizar el entorno virtual de __[Colaboratory - Google](https://colab.research.google.com/)__* . Así, podrá programar en la nube con recursos elevados y luego descargar el Jupyter Notebook y entregarlo en modo Informe. \n"]},{"metadata":{"id":"_kb7KYAZtFcg","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow\n","import seaborn as sns\n","import matplotlib.cm as cm\n","import os   \n","import keras as krs\n","import math\n","\n","\n","from keras.models import Sequential\n","from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, rmsprop\n","from keras.layers.core import Dense, Activation\n","from keras.layers import Dense, Activation\n","from keras import backend as bknd\n","from keras.datasets import cifar10\n","from keras import utils as np_utils\n","from keras.callbacks import LearningRateScheduler\n","from sklearn.preprocessing import StandardScaler\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.regularizers import l1,l2\n","import pickle\n","from google.colab import drive\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","from keras.datasets import cifar10, mnist"],"execution_count":0,"outputs":[]},{"metadata":{"id":"moWIc-yCmoOG","colab_type":"code","outputId":"0067fe00-a2e1-4b90-f5b2-d4dd2892f9e1","executionInfo":{"status":"ok","timestamp":1545649369364,"user_tz":180,"elapsed":6779,"user":{"displayName":"alumno regular","photoUrl":"","userId":"07871145298828266341"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","import pickle\n","\n","drive.mount('/content/drive', force_remount=True)\n","DATA_PATH = \"/content/drive/My Drive/Colab Tarea 3/Models/\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"N4yxv8_XtgNw","colab_type":"code","colab":{}},"cell_type":"code","source":["def Repartir_Conjuntos(Train,Test,x,y):\n","  print(\"This function splits the framework this way:\\n Train from 0% to (First parameter)%\\n Test from (First parameter)% to (First+Second parameter)%\\n Validation will be the rest.\")\n","  if (0<Train<1) == False:\n","    print(\"ERROR: First parameter must be type float between 0 and 1\")\n","    return\n","  if (0<Test<1) == False:\n","    print(\"ERROR: Second parameter must be type float between 0 and 1\")\n","    return\n","  if (Train+Test<1) == False:\n","    print(\"ERROR: First parameter plus Second parameter can't be bigger than one\")\n","    return\n","  \n","  largo = len(x);\n","  Train=int(Train*largo);\n","  Test = int(Test*largo + Train);\n","  (x_train, y_train)=x[0:Train],y[0:Train];\n","  (x_test, y_test)=x[Train:Test],y[Train:Test];\n","  (x_val, y_val)=x[Test:],y[Test:];\n","  return((x_train, y_train),(x_test, y_test),(x_val, y_val))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Jx-s_iBY-Qs","colab_type":"text"},"cell_type":"markdown","source":["<a id=\"segundo\"></a>\n","## 2. Autoencoders (AEs) en MNIST\n","\n","\n","Como se ha discutido en clases, las RBM’s y posteriormente los AE’s (redes no supervisadas) fueron un componente crucial en el desarrollo de los modelos que entre 2006 y 2010 vigorizaron el área de las redes neuronales artificiales con logros notables de desempeño en diferentes tareas de aprendizaje automático. En esta sección aprenderemos a utilizar el más sencillo de estos modelos: un autoencoder o AE. Consideraremos tres aplicaciones clásicas: reducción de dimensionalidad, *denoising* y pre-entrenamiento. Con este objetivo en mente, utilizaremos un dataset denominado MNIST[[3]](#refs). Se trata de una colección de 70000 imágenes de 28 $\\times$ 28 pixeles correspondientes a dígitos manuscritos (números entre 0 y 9). En su versión tradicional, la colección se encuentra separada en dos subconjuntos: uno de entrenamiento de 60000 imágenes y otro de test de 10000 imágenes. La tarea consiste en construir un programa para que aprenda a identificar correctamente el dı́gito representado en la imagen\n","\n","\n","> a) Escriba el código que **cargue los datos** desde el repositorio de keras, normalice las imágenes de modo que los pixeles queden en [0, 1], transforme las imágenes en vectores ($\\in {\\rm I\\!R}^{784}$) y devuelva tres subconjuntos disjuntos: uno de entrenamiento, uno de validación y uno de pruebas. Construya el conjunto de validación de la manera que estime conveniente, éste debe contar con $nval = 5000$ imágenes."]},{"metadata":{"id":"e4Xm8dA1ZEEr","colab_type":"code","outputId":"8906797f-8dba-4f22-9afa-e279aa312054","executionInfo":{"status":"ok","timestamp":1545649408067,"user_tz":180,"elapsed":1177,"user":{"displayName":"alumno regular","photoUrl":"","userId":"07871145298828266341"}},"colab":{"base_uri":"https://localhost:8080/","height":451}},"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x = np.concatenate((x_train,x_test), axis=0);\n","x = x.astype('float32') / 255.\n","y = np.concatenate((y_train,y_test), axis=0);\n","[[x_train, y_train],[x_test, y_test],[x_val, y_val]]=Repartir_Conjuntos(0.5,0.3,x,y)\n","print(\"-----------------------------------\")\n","print(\"x_train's shape: \",x_train.shape)\n","print(\"y_train's shape: \",y_train.shape)\n","\n","print(\"x_test's shape:  \",x_test.shape)\n","print(\"y_test's shape:  \",y_test.shape)\n","\n","print(\"x_val's shape:   \",x_val.shape)\n","print(\"y_vals's shape:  \",y_val.shape)\n","\n","x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n","x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n","x_val = x_val.reshape((len(x_val), np.prod(x_val.shape[1:])))\n","\n","print(\"-----------------------------------\")\n","print(\"x_train's shape: \",x_train.shape)\n","print(\"y_train's shape: \",y_train.shape)\n","\n","print(\"x_test's shape:  \",x_test.shape)\n","print(\"y_test's shape:  \",y_test.shape)\n","\n","print(\"x_val's shape:   \",x_val.shape)\n","print(\"y_val's shape:   \",y_val.shape)\n","\n","num_classes=10\n","y_train_categ = krs.utils.to_categorical(y_train, num_classes)\n","y_test_categ = krs.utils.to_categorical(y_test, num_classes)\n","y_val_categ = krs.utils.to_categorical(y_val, num_classes)\n","\n","print(\"-----------------------------------\")\n","print(\"x_train's shape      : \",x_train.shape)\n","print(\"y_train's categ shape: \",y_train_categ.shape)\n","\n","print(\"x_test's shape      :  \",x_test.shape)\n","print(\"y_test's categ shape:  \",y_test_categ.shape)\n","\n","print(\"x_val's shape      :   \",x_val.shape)\n","print(\"y_val's categ shape:   \",y_val_categ.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["This function splits the framework this way:\n"," Train from 0% to (First parameter)%\n"," Test from (First parameter)% to (First+Second parameter)%\n"," Validation will be the rest.\n","-----------------------------------\n","x_train's shape:  (35000, 28, 28)\n","y_train's shape:  (35000,)\n","x_test's shape:   (21000, 28, 28)\n","y_test's shape:   (21000,)\n","x_val's shape:    (14000, 28, 28)\n","y_vals's shape:   (14000,)\n","-----------------------------------\n","x_train's shape:  (35000, 784)\n","y_train's shape:  (35000,)\n","x_test's shape:   (21000, 784)\n","y_test's shape:   (21000,)\n","x_val's shape:    (14000, 784)\n","y_val's shape:    (14000,)\n","-----------------------------------\n","x_train's shape      :  (35000, 784)\n","y_train's categ shape:  (35000, 10)\n","x_test's shape      :   (21000, 784)\n","y_test's categ shape:   (21000, 10)\n","x_val's shape      :    (14000, 784)\n","y_val's categ shape:    (14000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"IwhUZP137P-P","colab_type":"text"},"cell_type":"markdown","source":["### 2.4 Pre-*training*\n","\n","En esta sección utilizaremos un AE para pre-entrenar redes profundas. Como hemos discutido en clases, el efecto esperado es regularizar el modelo, posicionando el modelo de partida en una buena zona del espacio de parámetros.\n","\n","> a) Construya y entrene una red FF para clasificar las imágenes de MNIST. Utilice SGD básico con tasa de aprendizaje fija $\\eta = 0.01$, momentum $m=0.9$ y no más de 50 *epochs*. Para empezar, utilice una arquitectura $768 \\times 1000 \\times 1000 \\times 10$ y **funciones de activación sigmoidales**. **Determine error de clasificación alcanzado por el modelo en el conjunto de test.**"]},{"metadata":{"id":"BQvXzkQRmZXf","colab_type":"code","outputId":"3c0d0794-d974-4fc2-bd6d-2f04a5f68c6e","executionInfo":{"status":"ok","timestamp":1545592038665,"user_tz":180,"elapsed":459488,"user":{"displayName":"alumno regular","photoUrl":"","userId":"07871145298828266341"}},"colab":{"base_uri":"https://localhost:8080/","height":2032}},"cell_type":"code","source":["from keras.utils import to_categorical\n","Y_train = to_categorical(y_train, 10)\n","Y_test = to_categorical(y_test, 10)\n","from keras.models import Sequential\n","model = Sequential()\n","model.add(Dense(1000, activation='sigmoid', input_shape=(784,)))\n","model.add(Dense(1000, activation='sigmoid'))\n","model.add(Dense(10, activation='softmax'))\n","model.summary()\n","optimizer_ = SGD(lr=0.01, momentum=0.9)\n","model.compile(optimizer=optimizer_,loss='categorical_crossentropy', metrics=['accuracy'])\n","history = model.fit(x_train, y_train_categ,nb_epoch=50, batch_size=25,shuffle=True, validation_data=(x_val, y_val_categ))\n","model.save(DATA_PATH+'T3_P2_4_a.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_45 (Dense)             (None, 1000)              785000    \n","_________________________________________________________________\n","dense_46 (Dense)             (None, 1000)              1001000   \n","_________________________________________________________________\n","dense_47 (Dense)             (None, 10)                10010     \n","=================================================================\n","Total params: 1,796,010\n","Trainable params: 1,796,010\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 35000 samples, validate on 14000 samples\n","Epoch 1/50\n","35000/35000 [==============================] - 10s 279us/step - loss: 0.9699 - acc: 0.6835 - val_loss: 0.4016 - val_acc: 0.8722\n","Epoch 2/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.3970 - acc: 0.8821 - val_loss: 0.3397 - val_acc: 0.8984\n","Epoch 3/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.3464 - acc: 0.8993 - val_loss: 0.3130 - val_acc: 0.9095\n","Epoch 4/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.3173 - acc: 0.9065 - val_loss: 0.2900 - val_acc: 0.9115\n","Epoch 5/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.2990 - acc: 0.9133 - val_loss: 0.2636 - val_acc: 0.9245\n","Epoch 6/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.2806 - acc: 0.9180 - val_loss: 0.2566 - val_acc: 0.9261\n","Epoch 7/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.2633 - acc: 0.9239 - val_loss: 0.2393 - val_acc: 0.9303\n","Epoch 8/50\n","35000/35000 [==============================] - 9s 263us/step - loss: 0.2498 - acc: 0.9267 - val_loss: 0.2458 - val_acc: 0.9311\n","Epoch 9/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.2362 - acc: 0.9309 - val_loss: 0.2099 - val_acc: 0.9374\n","Epoch 10/50\n","35000/35000 [==============================] - 9s 264us/step - loss: 0.2213 - acc: 0.9351 - val_loss: 0.2066 - val_acc: 0.9373\n","Epoch 11/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.2075 - acc: 0.9391 - val_loss: 0.2019 - val_acc: 0.9419\n","Epoch 12/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.1952 - acc: 0.9425 - val_loss: 0.1951 - val_acc: 0.9433\n","Epoch 13/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.1837 - acc: 0.9454 - val_loss: 0.1774 - val_acc: 0.9488\n","Epoch 14/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.1738 - acc: 0.9477 - val_loss: 0.1741 - val_acc: 0.9500\n","Epoch 15/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.1656 - acc: 0.9508 - val_loss: 0.1587 - val_acc: 0.9521\n","Epoch 16/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.1570 - acc: 0.9532 - val_loss: 0.1539 - val_acc: 0.9540\n","Epoch 17/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.1499 - acc: 0.9556 - val_loss: 0.1506 - val_acc: 0.9558\n","Epoch 18/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.1412 - acc: 0.9570 - val_loss: 0.1497 - val_acc: 0.9565\n","Epoch 19/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.1342 - acc: 0.9599 - val_loss: 0.1435 - val_acc: 0.9577\n","Epoch 20/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.1277 - acc: 0.9620 - val_loss: 0.1366 - val_acc: 0.9597\n","Epoch 21/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.1219 - acc: 0.9636 - val_loss: 0.1397 - val_acc: 0.9591\n","Epoch 22/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.1160 - acc: 0.9657 - val_loss: 0.1424 - val_acc: 0.9580\n","Epoch 23/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.1117 - acc: 0.9668 - val_loss: 0.1420 - val_acc: 0.9564\n","Epoch 24/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.1051 - acc: 0.9689 - val_loss: 0.1337 - val_acc: 0.9623\n","Epoch 25/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.1002 - acc: 0.9692 - val_loss: 0.1224 - val_acc: 0.9641\n","Epoch 26/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0963 - acc: 0.9713 - val_loss: 0.1256 - val_acc: 0.9629\n","Epoch 27/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.0919 - acc: 0.9725 - val_loss: 0.1178 - val_acc: 0.9641\n","Epoch 28/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0878 - acc: 0.9735 - val_loss: 0.1251 - val_acc: 0.9634\n","Epoch 29/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0835 - acc: 0.9752 - val_loss: 0.1160 - val_acc: 0.9661\n","Epoch 30/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.0802 - acc: 0.9758 - val_loss: 0.1104 - val_acc: 0.9674\n","Epoch 31/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.0772 - acc: 0.9775 - val_loss: 0.1154 - val_acc: 0.9659\n","Epoch 32/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0738 - acc: 0.9777 - val_loss: 0.1147 - val_acc: 0.9669\n","Epoch 33/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.0699 - acc: 0.9789 - val_loss: 0.1071 - val_acc: 0.9694\n","Epoch 34/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0669 - acc: 0.9797 - val_loss: 0.1128 - val_acc: 0.9664\n","Epoch 35/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0640 - acc: 0.9821 - val_loss: 0.1063 - val_acc: 0.9686\n","Epoch 36/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0607 - acc: 0.9819 - val_loss: 0.0997 - val_acc: 0.9694\n","Epoch 37/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0590 - acc: 0.9821 - val_loss: 0.1111 - val_acc: 0.9685\n","Epoch 38/50\n","35000/35000 [==============================] - 9s 258us/step - loss: 0.0555 - acc: 0.9836 - val_loss: 0.0979 - val_acc: 0.9714\n","Epoch 39/50\n","35000/35000 [==============================] - 9s 259us/step - loss: 0.0526 - acc: 0.9843 - val_loss: 0.0993 - val_acc: 0.9719\n","Epoch 40/50\n","35000/35000 [==============================] - 9s 258us/step - loss: 0.0507 - acc: 0.9852 - val_loss: 0.0991 - val_acc: 0.9722\n","Epoch 41/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0481 - acc: 0.9859 - val_loss: 0.0982 - val_acc: 0.9721\n","Epoch 42/50\n","35000/35000 [==============================] - 9s 263us/step - loss: 0.0460 - acc: 0.9868 - val_loss: 0.1001 - val_acc: 0.9687\n","Epoch 43/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0437 - acc: 0.9877 - val_loss: 0.0965 - val_acc: 0.9709\n","Epoch 44/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.0412 - acc: 0.9886 - val_loss: 0.0943 - val_acc: 0.9726\n","Epoch 45/50\n","35000/35000 [==============================] - 9s 260us/step - loss: 0.0390 - acc: 0.9889 - val_loss: 0.0956 - val_acc: 0.9733\n","Epoch 46/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0371 - acc: 0.9893 - val_loss: 0.0937 - val_acc: 0.9739\n","Epoch 47/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0349 - acc: 0.9905 - val_loss: 0.0983 - val_acc: 0.9711\n","Epoch 48/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0338 - acc: 0.9903 - val_loss: 0.1006 - val_acc: 0.9710\n","Epoch 49/50\n","35000/35000 [==============================] - 9s 261us/step - loss: 0.0321 - acc: 0.9910 - val_loss: 0.0940 - val_acc: 0.9734\n","Epoch 50/50\n","35000/35000 [==============================] - 9s 262us/step - loss: 0.0296 - acc: 0.9920 - val_loss: 0.0917 - val_acc: 0.9740\n"],"name":"stdout"}]},{"metadata":{"id":"vWxyupaCr0rH","colab_type":"code","outputId":"0d945474-f242-413f-96eb-b63c4175e908","executionInfo":{"status":"ok","timestamp":1545579647571,"user_tz":180,"elapsed":1468,"user":{"displayName":"alumno regular","photoUrl":"","userId":"07871145298828266341"}},"colab":{"base_uri":"https://localhost:8080/","height":671}},"cell_type":"code","source":["plt.figure(None, figsize=(10,10), dpi=80, facecolor='w', edgecolor='k')\n","plt.plot(history.history['loss'],'.r',label='train');\n","plt.plot(history.history['val_loss'],'.b',label='validation');\n","plt.ylabel('Loss');\n","plt.xlabel('Epoch');\n","plt.title('');\n","plt.legend();"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAqMAAAKOCAYAAACSrRCAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAMTQAADE0B0s6tTgAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3X+Y1nWdL/7nPTejDGiJQBCVlVsy\nQ+Gxn+7EUYNSoez7raslPZ0stx+21LGyjVO62Q8tc3N1dTXL2sKjKF26V1qxLMi2Yp1otrJaIgaT\nPX0rEwUEjOROcZjvHx4nKX7MDXPf75mbx+O69rr4zOeez+c98wJ77vtnpb+/vz8AAFBAW+kGAABw\n8BJGAQAoRhgFAKAYYRQAgGKEUQAAihFGAQAoRhgFAKCYUaUbcKA2btzW1PcdeeTYbN78cFPfSWOo\nZetQy9ahlq1DLVvHUNRy4sTD93hPz2gdKpWkWm1LpVK6JRwotWwdatk61LJ1qGXraEYthVEAAIoR\nRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAgIL+5V8WD/qzDz64KeefP7+B\nrWk+YRQAoJC+vr585StfGvTnx4+fkIsvvrSBLWq+EX82PQBAM1R716S9Z2V2dM9IX2fXkDzzM5+5\nMBs3PpD3v39eNm3amKlTu/KMZzwzb3nL2/LpT38yW7Zszu9/X8uxxx6Xc875YNavvy/vfvdf5hvf\nWJZPf/oTmTBhYv7zP9flvvvuTXf3f8173/v+IWlXMwmjAAD7UO1dk3GzZ6ZSq6W/oyNblt6Rvq5p\nB/zcd7zj3fn+93vykY9ckNNPf30uuuhvc/TRf5b77vtNXv7yP89pp/2/6e/vzxlnvCGvfe3/k46O\nMbt8/69+9f/lb//28tRq23PaaSfnrLPekbFjDzvgdjWTMAoAsA/tPStTqdWSJJVaLe09K4ckjD7Z\n2LGH5eij/yxJMn78+Nx999osXvz1jBo1Klu2bMnWrVv/JIy++MUvTaVSyZgxY3PYYYdn27ZtwigA\nQKvZ0T0j/R0dAz2jO7pnDPk72tvbB/58882L8rvfbcs11/xj2tractZZb97t91Sr1V2u+/v7h7xd\njSaMAgDsQ19nV7YsvWPI54xWKm3p63vsT77+4IOb8pznPDdtbW1ZvXpV1q//TXbs2DEk7xxurKYH\nABiEvq5p+f1fvnPIgmiSTJgwIRMnTsrb3/6WbNv224Gvn3ba67N8+bKcc86709OzMmec8ZZcccWl\neeihrUP27uGi0j8S+3OfZOPGbU17V6WSTJhweDZt2paR/VtDLVuHWrYOtWwdatk6hqqWEycevsd7\nekYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBib3teh2rsmWf2jVKe/JI9NHbo9xgAADlZ6\nRgep2rsmR5w6M3nPe3LEKa98PJgCADTJkiXfzMc/fl6S5OMfPy/333//n3zm4x8/L0uWfHOvz/np\nT/8j99776yTJlVdeljVrVg99Y+sgjA5Se8/KVGq1JEmlVkt7z8rCLQIAmqm3ty0LFrRn7dry8emT\nn/xMJk+evF/fu2TJNwfC6Pvf/9eZNu2FQ9m0uhmmH6Qd3TPS39GRSq2W/o6O7OieUbpJAECT9Pa2\nZfbsManVKuno6M/SpdvT1bXzgJ/7rne9Ne997wdy3HEvTpKcd95fZ+rUrvzwh99PtVrNtm3bcsYZ\n/z2nnDJnl+/7i794XS677KpMnjw5n/zkBdm4cUMmTZqUhx9+eOAz1133j+npWZlRo0bl0ENH58IL\nL84Pf/iD3HHHv2bNmtWpVtvyv/7XV/KWt5yVP//zV2Thwuvy7W+vyCGHHJKnPW1SPvSh8zJ27Ji8\n6EUvyjve8e6sXPm/s2HDA/nwhz860N6hUD7ajxB9nV3ZuuyO5JprsvX2FUN6Li0AMLz19FRTq1WS\nJLVaJT091SF57sknz8kdd/xrkuThh3+Xn/50VaZO7cpZZ70zV175+Vx88d/lyiv/bo/ff/vtS1Op\nJF/60v/Keed9LPfc8/MkyWOPPZa2trZcffUXc/XVX8z48eOzdOk/56STZuZ5zzsm8+a9Ly972Z8P\nPOdnP1ud22//l3zuc1/K1Vd/MWPHHpZ/+qevJkm2b9+eZzzjGbnyys/n9NP/e265ZdGQ/OxP0DNa\nh76uackJx6dv07bEWbsAcNDo7u5LR0f/QM9od3ffkDz31a8+Je9851vz/vd/KN/+9orMmHFCjjrq\n2bnmmivzla98MZVKJQ899NAev3/dup/n2GOPS5KMHXtYpk17QZJk1KhRGT26I+ec8+5Uq9Xcd99v\nMmnSnof1f/azVTnuuBenvb09SfLiF78kt9/+LwP3X/KSlydJJk9+en77298e8M/9ZMIoAMA+dHbu\nzNKl29PTU013d186Ow98iD5JjjxyfJ7znKOzatVP8m//tjxvfvNbc9lll+TEE1+Z17/+L7Jt27bM\nmTNzj9/f39+fSuUPA919fY+3a/XqVfna127JV75yQ8aMGZsrr7xsHy2p/NFzd/1atVp90r2h7ZEz\nTA8AMAhdXTvzl3+5Y8iC6BNOOWV2li37l/z617/Occe9OA8+uClHH/28JMmyZf+cSqWSRx99dLff\n+9zn/ll++tP/SJJs27Ytvb0/S5I8+OCmTJ48OWPGjM2WLVvygx/8e3bs2JEkaWtrS1/frj27L3zh\n9Pz4x3cNfOYHP+jJ9OnHDunPuSd6RgEACjrxxJm5/PLPZu7cM1KpVPLmN781n/nMhZk0aXJe97o3\n5MUvflk+/vHzcsIJr/yT7z311Nfke9/735k37+2ZMOFpeeELHw+QL395d26+eVHmzXt7Jk+ekne9\na14uu+ySHH98d172sj/PZZddklpt+8Bzpk17YebMOS3vfe+7Uq1W88xnPitvfOObmvLzV/qHuq+1\nyTZu3Na0d1UqyYQJh2fTpm0Z2b811LJ1qGXrUMvWoZatY6hqOXHi4Xu8Z5geAIBihFEAAIoRRgEA\nKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEA\nAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEU\nAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYY\nBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoR\nRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBi\nhFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCg\nGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBihFEAAIoRRgEA\nKGZUs1507bXXZvny5alWqzn22GNz/vnnp1KpDNxfuHBhFi9enFGjRuXQQw/Npz71qTz96U9vVvMA\nACigKT2jq1atyuLFi7Nw4cIsWrQo69aty/LlywfuP/DAA/nyl7+cG264IQsXLsyLXvSifP7zn29G\n0wAAKKgpPaN33nlnZs2aldGjRydJ5syZkxUrVuSUU05JkowePTqVSiW/+93vMm7cuDz00EM58sgj\nB/38J3WwNtQT72nW+2gctWwdatk61LJ1qGXraEYtmxJGN2zYkKlTpw5cT5w4MQ888MDA9VOf+tS8\n733vy6tf/eqMHz8+Y8eOzY033jioZx955NhUq82d+jp+/OFNfR+No5atQy1bh1q2DrVsHY2sZdPm\njD5Zf3//Ltf33XdfrrrqqixZsiSTJk3KNddck89+9rP5xCc+sc9nbd78cFN7RsePPzwPPrgtf/Qj\nMMKoZetQy9ahlq1DLVvHUNVywoQ9h9mmhNHJkydnw4YNA9fr16/PlClTBq5/8pOfpLOzM5MmTUqS\nzJo1K3/913896Oc3+y96f3/z30ljqGXrUMvWoZatQy1bRyNr2ZTx7ZkzZ+Zb3/pWarVaHnvssSxZ\nsiSvfvWrB+4fffTRufvuu1Or1ZI8Hk7/7M/+rBlNAwCgoKb0jE6bNi1z587NmWeemba2tnR3d+ek\nk07Kueeem/nz56ezszNve9vb8ta3vjWjR4/O6NGj88lPfrIZTQMAoKBK/x9P4BxhNm7c1rR3VSqP\nz3nYtMkcmJFOLVuHWrYOtWwdatk6hqqWEyfuec6oE5gAAChGGAUAoBhhFACAYoRRAACKEUYBAChG\nGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACK\nEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACA\nYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUA\noBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYB\nAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRR\nAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhh\nFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChG\nGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACK\nEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoBhhFACA\nYoRRAACKEUYBAChGGAUAoBhhFACAYoRRAACKEUYBAChGGAUAoJhRzXrRtddem+XLl6darebYY4/N\n+eefn0qlMnD/17/+dc4777w8+uijaWtry5VXXplJkyY1q3kAABTQlJ7RVatWZfHixVm4cGEWLVqU\ndevWZfny5bt85vzzz8/cuXNz8803541vfGNWrFjRjKYBAFBQU3pG77zzzsyaNSujR49OksyZMycr\nVqzIKaeckiTZvHlz1q5dm9e+9rVJkrlz5zajWQAAFNaUMLphw4ZMnTp14HrixIl54IEHBq5//etf\nZ9KkSbnmmmvS09OT8ePH56Mf/eigh+mfNNrfUE+8p1nvo3HUsnWoZetQy9ahlq2jGbVs2pzRJ+vv\n7/+Tr91777057bTT8r73vS/XXHNNPvWpT+Wqq67a57OOPHJsqtXmrsMaP/7wpr6PxlHL1qGWrUMt\nW4dato5G1rIpYXTy5MnZsGHDwPX69eszZcqUgeunPe1pmTRpUo4++ugkySmnnJJbb711UM/evPnh\npvaMjh9/eB58cFt2k6cZQdSydahl61DL1qGWrWOoajlhwp7DbFPC6MyZMzN//vzMmzcv7e3tWbJk\nSc4+++yB+09/+tPzlKc8JT//+c9zzDHH5Mc//vEuw/r70uy/6P39zX8njaGWrUMtW4datg61bB2N\nrGVTwui0adMyd+7cnHnmmWlra0t3d3dOOumknHvuuZk/f36mTJmSSy65JB/96EfT1taWQw45JBdd\ndFEzmgYAQEGV/t1N4BxBNm7c1rR3VSqPdzNv2mTYYaRTy9ahlq1DLVuHWraOoarlxIl7HqZ3AhMA\nAMUIowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAK\nAEAxwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOM\nAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMUI\nowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAKAEAx\nwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMXUFUYfe+yxgT//8pe/zK9+9ashbxAAAAePQYfRm266\nKeeee+7An9/0pjflHe94R6677rpGtQ0AgBY36DB6/fXX56KLLkqSXHvttfnyl7+cb37zm7nlllsa\n1jgAAFrbqMF+sL29PUcccUTWrFmTQw89NC984Qsb2S4AAA4Cgw6jY8eOzW233Zbbb789c+bMSZKs\nW7cu1Wq1YY0DAKC1DXqY/qKLLsodd9yRyZMn56/+6q+SJJdeemnmz5/fsMYBANDaBt0z+vznPz9X\nXnnlwHWtVss//MM/5NBDD21IwwAAaH2D7hldvnz5QC/osmXL8opXvCInnnhiFi9e3LDGAQDQ2gYd\nRq+44oqcc845SZLLL788V199dZYuXZprr722YY0DAKC1DXqYPkmOOuqo/J//83/y+9//PjNmzEiS\n9Pf3N6RhAAC0vkGH0UMPPTQ//OEPc9ttt+Xkk09OkmzcuDE7d+5sWOMAAGhtgx6m/8hHPpJLLrkk\nGzduzHve854kyQc/+MGcffbZDWscAACtbdA9oy9/+cvzT//0T7t8bcGCBRk1qq6RfgAAGDDoJLll\ny5Zcfvnl+e53v5sHH3wwEyZMyKxZs/L+978/hx12WCPbCABAixr0MP3HP/7xJMmVV16ZxYsX54or\nrsjvfve7XHjhhQ1rHAAArW3QPaPr1q3LkiVLBq6f9axn5eKLL85rXvOahjQMAIDWN+ie0f7+/jz8\n8MO7fG379u2pVCpD3igAAA4Og+4ZnTNnTs4444y8/vWvz7hx47J169bcdtttee1rX9vI9gEA0MIG\nHUbf97735XnPe17uvPPOgQVM733ve3Pqqac2sn0AALSwuvZles1rXvMnc0Rvv/32nHLKKUPaKAAA\nDg6DnjO6J1dcccVQtAMAgIPQAYdRZ9MDALC/DjiMWk0PAMD+2uec0fvuu2+v9x977LEhawwAAAeX\nfYbRWbNmpVKp7HE4Xs8oAAD7a59hdO3atc1oBwAAB6EDnjMKAAD7SxgFAKAYYRQAgGKEUQAAihFG\nAQAoRhgFAKAYYRQAgGKEUQAAihFGAQAoRhgFAKAYYRQAgGKEUQAAihFGAQAoRhgFAKAYYRQAgGKE\nUQAAihFGAQAoRhgFAKAYYRQAgGKEUQAAihFGAQAoZlTpBowkvb1tWb06mT69LVOn7izdHACAEU8Y\nHaTe3rbMnj0mtVrS0TEmS5duT1eXQAoAcCAM0w9ST081tVolSVKrVdLTUy3cIgCAkU8YHaTu7r50\ndPQnSTo6+tPd3Ve4RQAAI59h+kHq7NyZZcu2Z/XqsZk+fbs5owAAQ0AYrUNX186ccEKyadPO9PeX\nbg0AwMhnmB4AgGKEUQAAihFGAQAoRhgFAKAYYRQAgGKEUQAAihFGAQAoRhgFAKAYYRQAgGKaFkav\nvfba/MVf/EVOP/30fPrTn07/Ho4w+vKXv5xZs2Y1q1kAABTUlDC6atWqLF68OAsXLsyiRYuybt26\nLF++/E8+d8899+Q73/lOM5oEAMAw0JQweuedd2bWrFkZPXp02traMmfOnKxYsWKXz+zYsSMXXHBB\nPvaxjzWjSQAADAOjmvGSDRs2ZOrUqQPXEydOzAMPPLDLZz73uc9l9uzZOfroo+t+fqVywE2s6z3N\neh+No5atQy1bh1q2DrVsHc2oZVPC6B/74/miq1atyo9//OMsWLCg7mcdeeTYVKvNXYc1fvzhTX0f\njaOWrUMtW4datg61bB2NrGVTwujkyZOzYcOGgev169dnypQpA9dLlizJ5s2bc8YZZyR5vCf1rW99\na66//vp9Pnvz5oeb2jM6fvzhefDBbdnD+qsB1d41ae9ZmR3dM9LX2dWcBjJo9dSS4U0tW4datg61\nbB1DVcsJE/YcZpsSRmfOnJn58+dn3rx5aW9vz5IlS3L22WcP3P/IRz6yy+dnzZo1qCD6hGb/Re/v\n3/s7q71rcsTsmanUaunv6MiWpXekr2ta8xrIoO2rlowcatk61LJ1qGXraGQtmzK+PW3atMydOzdn\nnnlm3vzmN+elL31pTjrppJx77rm57777mtGEpmrvWZlKrZYkqdRqae9ZWbhFAADDU6V/Txt+jhAb\nN25r2rsqlce7mTdt2ntXdXVtb8ad+so/9IwuW2GofpgZbC0Z/tSydahl61DL1jFUtZw4sfAw/cGm\nr7MrW5beYc4oAMA+CKMN0tc1zTxRAIB9cDY9AADFCKMAABQjjAIAUIwwCgBAMcIoAADFCKMAABQj\njAIAUIwwCgBAMcJog/T2tmXBgvasXetXDACwJ05gaoDe3rbMnj0mtVolHR39Wbp0e7q6dpZuFgDA\nsKPbrgF6eqqp1SpJklqtkp6eauEWAQAMT8JoA3R396Wjoz9J0tHRn+7uvsItAgAYngzTN0Bn584s\nXbo9PT3VdHf3pbPTED0AwO4Iow3S1bXTPFEAgH0wTA8AQDHCKAAAxQijAAAUI4wCAFCMMAoAQDHC\nKAAAxQijAAAUI4wOE729bVmwoD1r1yoJAHDwsOn9MNDb25bZs8ekVquko6M/S5dut2E+AHBQ0A03\nDPT0VFOrVZIktVolPT3Vwi0CAGgOYXQY6O7uS8ehfUmSjkP70t3dV7hFAADNIYwOAy/oX51/z/G5\nJvPy/bw8L+hfXbpJAABNYc7oMNDeszLTH7kr03NX8kiyrWdl+rqmlW4WAEDD6RkdBnZ0z0h/R0eS\npL+jIzu6ZxRuEQBAc+gZHQb6OruyZekdae9ZmR3dM9LX2VW6SQAATSGMDhN9XdMMzQMABx3D9AAA\nFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMAgBQjDAKAEAxwigAAMUIowAAFCOMjkDV3jUZveAfU13b\nW7opAAAHxHGgI0y1d03GzZ6ZSq2W/o6ObFl6h2NEAYARS8/oCNPeszKVWi1JUqnV0t6zsnCLAAD2\nnzA6wuzonpH+jo4kSX9HR3Z0zyjcIgCA/WeYfoTp6+zKlqV3pL1nZXZ0z0hfZ1fpJgEA7DdhdATq\n65pmnigA0BIM0wMAUIwwCgBAMcIoAADFCKMAABQjjAIAUIwwCgBAMcLoQcBZ9gDAcGWf0RbnLHsA\nYDjTM9rinGUPAAxnwmiLc5Y9ADCcGaZvcc6yBwCGM2H0IOAsewBguDJMDwBAMcIoAADFCKMjUG9v\nWxYsaM/atcoHAIxs5oyOML29bZk9e0xqtUo6OvqzdOn2dHXtLN0sAID9omtthOnpqaZWqyRJarVK\nenqqQ/p8pzUBAM2kZ3SE6e7uS0dH/0DPaHd335A922lNAECzCaMjTGfnzixduj09PdV0d/els3Pf\nQ/S9vW2D+vzuTmsSRgGARhJGR6Curp2DnidazxzTJ05reqJn1GlNAECjCaMtbndzTPcURp3WBAA0\nmzDa4uqdY+q0JgCgmYTRFrc/c0wBAJpFGD0I1DPHFACgmewzCgBAMcIoAADFCKMcECc2AQAHwpxR\ndjHYDfITJzYBAAdOGGVAPRvkJ05sAgAOnGF6Buxug/y9eeLEpiRObAIA9oueUQbUvUG+E5sAgAMk\njDJgfzbId2ITAHAghFF2YYN8AKCZzBkFAKAYYRQAgGKEUQAAihFGaRqnNQEAf8wCJprCaU0AwO7o\nGaUpdndaEwCAMEpTOK0JANgdw/Q0hdOaAIDdEUZpGqc1AQB/zDA9AADFCKMckN7etixY0J61a/1V\nAgDqZ5ie/dbb25bZs8ekVquko6M/S5duH9Jz7au9a8wxBYAWJ4yy33p6qqnVKkmSWq2Snp7qkIVR\n+5ICwMHB2Cr7rbu7Lx0d/UmSjo7+dHf3Ddmz7UsKAAcHPaPst87OnVm6dHt6eqrp7u5LZ+fQDdE/\nsS/pEz2j9iUFgNYkjHJAurp2Duk80SfYlxQADg7CKMOWfUkBoPWZMwoAQDHCKE1jT1IA4I8Zpqcp\n7EkKAOyOMEpT2JMUANgd46U0hT1JAYDd0TNKU9iTFADYHWGUprEnKQDwx4RRhq3e3rZB96TakxQA\nRiZhlGGp0avvAYDhwQImhqXdrb4HAFqPMMqw1MjV98nj20Hl859PdW3vkD4XAKiPYXqGpXpX39cz\nv7TauyZHzJ6Z1Go5wr6kAFCUMMqwNdjV9/XOL93dvqTCKACU0bQweu2112b58uWpVqs59thjc/75\n56dSqQzcv/XWW7Nw4cIccsghOfzww/PZz342RxxxRLOaxwhW7+lO9iUFgOGjKXNGV61alcWLF2fh\nwoVZtGhR1q1bl+XLlw/cv++++3LZZZfluuuuy6JFi/Lc5z431113XTOaRguod35pX2dXti67I7nm\nmmy9fYV9SQGgoKb0jN55552ZNWtWRo8enSSZM2dOVqxYkVNOOSVJ8vSnPz1Lly7NYYcdliQZP358\nfvOb3zSjabSA/Tndqa9rWnLC8enbtC3pb0IjAYDdakoY3bBhQ6ZOnTpwPXHixDzwwAMD15VKZSCI\nbtmyJbfccksuvfTSQT//SaP9DfXEe5r1PgZv2rSdmTZt8PuQqmXrUMvWoZatQy1bRzNqWWQBU3//\n7rui1q9fn7PPPjvvec97ctxxxw3qWUceOTbVanN3qBo//vCmvo+ht3p1cvPNyYknHp4XvKB0axgK\n/l22DrVsHWrZOhpZy6aE0cmTJ2fDhg0D1+vXr8+UKVN2+cx9992Xt7/97Zk/f35e9apXDfrZmzc/\n3NSe0fHjD8+DD27LHvI0I0Bvb1tOPfUPq++XLdv76vtq7xrn3g9j/l22DrVsHWrZOoaqlhMm7DnM\nNiWMzpw5M/Pnz8+8efPS3t6eJUuW5Oyzzx64v3Pnzpxzzjm54IILMmNG/Subm/0Xvb+/+e9k6Hzv\ne7uuvv/e96p7nGda7V2Te08YOeVaAAAV1klEQVR5b/73Iy/LCYcuzJTbr7EN1DDl32XrUMvWoZat\no5G1bEoYnTZtWubOnZszzzwzbW1t6e7uzkknnZRzzz038+fPzy9+8Yv853/+Z77whS/kC1/4QpLk\nmGOOyQUXXNCM5nGQeWL1/RM9o3tbfX/PbXdn5iPfTi1j0vHI9txx2z/laGEUAIZM0+aMnnXWWTnr\nrLN2+drf//3fJ0mmTJmSn/zkJ81qCge5zs6dWbZse1avHpvp07dn6tQ9D9F/p3JiahmTJKllTL5T\nOTFHN6uhAHAQcAITB6Wurp054YRk06adex12OP71E9Pxub7UHqmm49C+HP/6iUkGv2ofANg7YRT2\norNzZ5be/vu69jC14AkABk8YhX3o6tq519X2T1btXZNxs2cOHDW6ZekdFjwBwF40d4NOaHHtPStT\nqdWSJJVaLe09Kwu3CACGN2EUhtCO7hnp7+hIkvR3dGRHd/1blQHAwcQwPQyhvs6ubFl6hzmjADBI\nwigMsb6uaeaJAsAgGaaHIdbb25YFC9qzdu2+/3lVe9dk9IJ/THVtbxNaBgDDj55RGEK9vW2ZPfsP\n594vXbrnc++tvAcAPaMwpHp6dj33vqenusfPtveszM9qR+fz+ausqT3XynsADkp6RmEI1XPu/apJ\nr87Jeffj595ne5ZP+s8c08S2AsBwIIzCEOrs3JmlS7cP6sSm7z7w/NQyOsnj595/94Hn55jsaFZT\nAWBYEEZhiA32xKZ6elGf0NvbVtfRpAAw3AmjUEg9vajJ/10cdcro1B6ppuPQviy9/feDPqYUAIYr\nYRQKqufc++/ftiG1R56bJKk9Us33b9uQrq4JjWweADSc1fQwQpyY76Qj25MkHdmeE/Odwi0CgAMn\njMII8bw3dObfDz0x12Revn/oCXneGzpLNwkADphhehgh+jq78ozbP5e39KzMju6/dO49AC1BGIUR\npJ5z73/+z7/I97++Mce/4Wl5/pznNLZhALCfhFFoQT//51/k5L98Xmo5Nh23bc/yBetyzGufW7pZ\nAPAnzBmFFvT9r29MLWOSPL6h/ve/vnGf39Pb25YFC9qzdq3/LADQPHpGoQUd/4anpeO27QNHjR7/\nhqft9fO9vW2ZPXvMwAb8S5dut4cpAE0hjEILev6c52T5gnWDnjPa01NNrVZJktRqlfT0VPcaRp0E\nBcBQEUahRR3z2ucOep7of53083TkeQM9qf910roku/9evagADCWTw4BMf+Bb+X5enmsyLz/IyzL9\ngW/t8bO760XdF/NRAdgTPaNAdnTPyAs6/iYvrP0s/R0d2dI9Y4+f7e7uS0dH/0DPaHd3316fXW9P\nqikAAAcXYRRIX2dXtiy9I+09K7Oje8ZeN9Tv7NyZpUu3Dzow1jMf1RQAgIOPMAokqW9D/RdmdV6U\nldmRGenL3k+Cqqcntd6FVACMfMIoUJdq75qMmz0zlVrt8SH9pXfsNcTW05Na7xQAAEY+YRSoS3vP\nylRqtSRJpVZLe8/KffaodnXtHFQPZ71TAAAY+YRRoC47umekv6NjoGd0x14WOz2h2rtmUPNRk8EH\nVwBagzAK1KWexU5J/cP6jTScVuoPp7YAlCSMAnWrZ7HT/gzr16O3ty2rVyfTp7dl6tS9bxlV70r9\nRgVGuwYA/IEwCjTU/gzrD9YfQl3S0TFmr6Fuf448bVRgtGsAwB84DgVoqCeG9bf97eXZsmzFoIb1\nRy/4x1TX9u7z2fWcBvXESv0kg1qpvz8nTQ1WvW0BaGV6RoGGG+ywfr3zS+vZCqrelfqN3GbKrgEA\nfyCMAsNGvfNLOzt3Ztmy7Vm9emymT9++1zmjSX0r9esNjPXOL7VrAMDjhFFg2Nif+aVdXTtzwgnJ\npk07098/tO0ZbGC0IAlg/wmjwLBR77ZRyeND+1n9o1SnvySPTd335xvBgiSA/SeMAsNKPdtGVXvX\n5IjZM5NaLUcU3MPUMaYA+08YBUasRu9hOlgjeUGSzfeB0oRRYMSqd45pPceS1quRC5LqDYyD/Xyj\n57oKusBgCKPAiNXX2ZWty+7IuNU/ytbpL0nfXuaMDqdjSetRb2Cs5/ONnOtqURcwWDa9B0a0vq5p\nybx5++zp3N2Q/khQ7+b7jTwIIHk8ZC5Y0J61a/f+Px+NPDQAaC16RoGDwv5sG9XIYf3BqndxVCMP\nAqint9OiLmCwhFHgoFDvtlHDZVi/3sBY7+frmetaz7B+oxd1mY8KrUMYBQ4a9WwbNVxW6if1L45q\n1GKqens7G9WO/ZmPKrw2V29vW1avTqZPb9vnyWggjALsxnBaqT9cNKO3czABpt6FVxZTNdcfft9J\nR8cYv2/2SRgF2I16hvWHy5B+MzS+t3PfAabeHlonZDWX3zf1EkYB9mCww/r7M6R/MPSk1qOR81Et\npmouv2/qJYwCHKD9GdI/WHpSB6uR81Etpmquzs6dWbZse1avHpvp07ebM8o+CaMAB6jelfrDaXHU\ncNHoAFNPeK0nXB4sp1jV246urp054YRk06ad6e8f2mfTeoRRgCFQz0r9/dnz9GBQT4BplHrD5Ug+\nxWo4HBtrcRmJE5gAmu6JntRtf3t5tixbMag9T0cv+MdU1/Y2qYUHr3pPjqr3FKvBnmC1P22p59lP\nhMAPf3h0Tj11THp79/w9jTxNa7id1FXP75Cho2cUoIDB9qSaX9pc9c5drWc+ar29gPW0pZE9uo1c\nkDScFjsdLL20w3FahDAKMIxZqd9c+7PYabDzUesd0q+nLfU+u5HHxtZjf57dqDDV6C2phkMIHK6B\nWxgFGMas1G++4XKCVT1taWSPbj3t2B/1Li6rJ0zVEwAb2Us7XOb/Dtc9YIVRgGHMSv3WMdx6GBsZ\nMBulnjBVbwCs93dYT9Ddn1PDGrGjw3CaFvFkwijAMNfIlfqG9JtruPQwjlT1hKn96QUc7O9wpM7/\nbfSeu/tLGAVoIY4xpZXVE6Ya2Qs4Uuf/JsPz/2kRRgFaTCOPMYXSBhumGtkLOJLn/w5HwijAQWp/\nNt83rM9I0qheQPN/h5YwCnCQqndxlGF9+APzf4eOMApwEKtncVS9w/p6UYHBEEYBGJR6hvX3pxe1\n2rsmWf2jVKe/JI9NFV7hYCGMAjAo9Qzr708v6hGzZya1Wo4wBQAOKsIoAIM22GH9ehdHWdkPBy9h\nFIAhV+/iKJv1w8FLGAWgIepZHNXX2ZWty+7IuNU/ytbpL0nfXuaMWtUPrUUYBWBY6OualpxwfPo2\nbUv69/y5/RnS15MKw5cwCsCIsj9D+vX0pAqu0FzCKAAjSr3zUevpSTUFAJpPGAVgxKlnPmo9Pamm\nAEDzCaMAtLR6elIbPQUA+FPCKAAtb7A9qY2cApDoRYXdEUYB4EkaNQVALyrsnjAKAPupkUekJnpS\nOTgIowBwABp1RKqeVA4WwigANIH5qLB7wigANMlwmo8qvDJcCKMAMAw1cj6qU6kYToRRABimGjUf\n1alUDCfCKACMcPXOR23GqVRZ/aNUp78kj03Vk8reCaMA0ALqmY/a6FOpjpg9M6nVcoSeVAZBGAWA\ng5BTqRguhFEAYK/sAkAjCaMAwJDp6+zK1mV3ZNzqH2Xr9Jekby9zRu0CQCKMAgBDrK9rWnLC8enb\ntC3p3/PnhtsuAMJrGcIoAFDEcNoFQK9rOcIoAFDMcNkFwN6r5QijAMCI0ahdAJqx96qe1N0TRgGA\nljRcel1NAdg7YRQAIMNj79WDceGVMAoAUKdG7b16MC68EkYBABrIwqu9E0YBABpspC68agZhFABg\nGBkuC6+aRRgFABjBGtXr2izCKADAQaKeXtdmaSvdAAAADl7CKAAAxQijAAAUI4wCAFCMMAoAQDHC\nKAAAxQijAAAUI4wCAFCMMAoAQDHCKAAAxQijAAAUI4wCAFDMqGa96Nprr83y5ctTrVZz7LHH5vzz\nz0+lUhm4f+utt+bGG2/MqFGj8sxnPjMXX3xxDjnkkGY1DwCAAprSM7pq1aosXrw4CxcuzKJFi7Ju\n3bosX7584P7999+fK664Il/60pfy1a9+Ne3t7bnxxhub0TQAAApqShi98847M2vWrIwePTptbW2Z\nM2dOVqxYMXB/5cqVednLXpZx48YlSU477bRd7gMA0JqaMky/YcOGTJ06deB64sSJeeCBB3a5P3Hi\nxF3u33///YN+/pNG+xvqifc06300jlq2DrVsHWrZOtSydTSjlk2bM/pk/f39+7xfGeRPfeSRY1Ot\nNncd1vjxhzf1fTSOWrYOtWwdatk61LJ1NLKWTQmjkydPzoYNGwau169fnylTpuxy/5577hm4vv/+\n+3e5vzebNz/c1J7R8eMPz4MPbss+8jTDnFq2DrVsHWrZOtSydQxVLSdM2HOYbUoYnTlzZubPn595\n8+alvb09S5Ysydlnnz1wf8aMGbniiiuyefPmHHnkkfnGN76RV73qVYN+frP/ovf3N/+dNIZatg61\nbB1q2TrUsnU0spZNCaPTpk3L3Llzc+aZZ6atrS3d3d056aSTcu6552b+/PmZMmVKPvShD+Vd73pX\n2tvb8/znPz+nn356M5oGAEBBlf59TeAc5jZu3Na0d1Uqj3czb9pk2GGkU8vWoZatQy1bh1q2jqGq\n5cSJex6mdwITAADFCKMAABQjjAIAUMyInzMKAMDIpWcUAIBihFEAAIoRRgEAKEYYBQCgGGEUAIBi\nhFEAAIoRRgEAKGZU6QaMFNdee22WL1+earWaY489Nueff34qlUrpZlGH3/72t/nYxz6WH/zgB/nu\nd7+bJLnzzjtz9dVXp729PYcffng++9nP5qlPfWrhlrI3X/ziF7Ns2bJUq9UcddRRufjii/O9731P\nHUeYnTt35tJLL81dd92VUaNGZfz48fnMZz6Tu+66Sy1HsAsvvDD33HNPbrjhhtx666258cYbM2rU\nqDzzmc/MxRdfnEMOOaR0E9mLf//3f8973/vedHV1DXztk5/8ZP7jP/6jobXUMzoIq1atyuLFi7Nw\n4cIsWrQo69aty/Lly0s3izp98IMfzPHHHz9w/cgjj+Rv/uZvctlll+Wmm27K9OnTc9VVVxVsIfty\n11135Zvf/Ga++tWv5uabb84jjzySW265RR1HoB/96EfZsGFDbr755tx0003p6OjI9ddfr5Yj2MqV\nK3P33XcnSe6///5cccUV+dKXvpSvfvWraW9vz4033li4hQxGV1dXbrjhhoH/GzNmTMNrKYwOwp13\n3plZs2Zl9OjRaWtry5w5c7JixYrSzaJOf//3f58TTjhh4PonP/lJnvWsZ+Woo45Kkpx22mnqOswd\nd9xxWbRoUdrb25Mk48aNy8MPP6yOI9BLX/rSXHbZZUmSRx99NBs2bMhznvMctRyhtm3blr/7u7/L\nRz7ykSSPB9OXvexlGTduXBK1HMmaUUthdBA2bNiQiRMnDlxPnDgxDzzwQMEWsT8OP/zwXa53V9f7\n77+/2c2iDtVqNYcddliS5Je//GVWrFiRnTt3quMI9tnPfjazZs3K8573vPT19anlCHXRRRdl3rx5\nA4HFf19Hrt/85jc555xzcvrpp+fSSy/Nfffd1/BaCqP7ob+/v3QTaID+/n7zgEeItWvX5p3vfGcu\nvvjiPOMZz9jlnjqOLP/zf/7P/Nu//VsefPDB3HvvvbvcU8uR4fbbb09/f39OPvnkPX5GLUeG5zzn\nOXnf+96Xyy67LNdff33Wrl2barW6y2caUUsLmAZh8uTJ2bBhw8D1+vXrM2XKlIItYig8/elP36Wu\n999/v7qOAGvWrMkHPvCBXHrppTnuuOPywx/+UB1HoHvuuSd9fX3p7OzMIYccktmzZ+emm27KI488\nMvAZtRwZlixZkl/+8pd505velEcffTS/+tWv8sMf/jCvec1rBj6jliPDpEmT8vrXv37getasWVmw\nYEH+y3/5LwNfa0Qt9YwOwsyZM/Otb30rtVotjz32WJYsWZJXv/rVpZvFATr22GOzfv36/OIXv0iS\nfP3rX8+rXvWqwq1ib7Zv355zzz03V111VY477rgk6jhSrVu3LhdeeGEee+yxJI8vaOrs7FTLEeiK\nK67IrbfemptvvjlXX311XvCCF+Tb3/527rrrrmzevDlJ8o1vfEMtR4Dbbrstl19+eZLHe0B7enry\nxje+seG11DM6CNOmTcvcuXNz5plnpq2tLd3d3TnppJNKN4s6bN26Neecc04eeeSRPPTQQznzzDNz\nzDHH5JJLLsmHP/zhVKvVTJw4MRdffHHpprIXixcvztatW/OpT31q4GuveMUr1HEEmj17dn72s5/l\nv/23/5ZqtZoJEybk05/+dF75yleqZQuYOHFiPvShD+Vd73pX2tvb8/znPz+nn3566WaxDyeffHLO\nO++8nH766env788LXvCCvPOd78yznvWshtay0m8CJAAAhRimBwCgGGEUAIBihFEAAIoRRgEAKEYY\nBQCgGFs7AQyhqVOn5lnPelZGjdr1P68f+MAHMnv27CF919e+9rXceuutueGGG4b0uQDNJIwCDLHr\nrrsuz3zmM0s3A2BEMEwP0CT33ntvpk+fnhtuuCGve93r8opXvCK33HLLwP2bbropc+bMyezZs3PW\nWWfl17/+dZLk0Ucfzfnnn59Zs2bl1FNPzfXXX7/Lcy+55JKceuqpOfnkk9PT09PUnwngQAmjAE30\n6KOP5re//W2++c1v5itf+UouvPDCbNmyJatWrcrVV1+d6667LkuXLs3LX/7yXHDBBUmSL3/5y9m6\ndWv+9V//NV/96lfzhS98IT/72c+SJD/96U9z8sknZ9myZZk7d24+97nPlfzxAOpmmB5giJ111ll/\nMmf0G9/4xsCfnzhKr7OzM0cddVRWrVqVn/zkJznllFMyadKkJMkZZ5yRq666Ko899ljuvPPOnHXW\nWWlra8u4ceOyfPnyjBkzJnfffXee/exn5yUveUmSZPr06bn55pub9FMCDA1hFGCI7WvO6Lhx4wb+\n/JSnPCW//e1v8+CDD2b8+PEDX3/qU5+anTt3ZsuWLdmyZUue8pSnDNwbO3bsLt//hLa2tuzcuXOo\nfgyApjBMD9BkW7ZsGfjzQw89lKc+9amZMGFCtm7dusvXq9Vqxo0bl3HjxmXz5s0D9zZu3Jjf/e53\nTW0zQKMIowBNduuttyZJent7c++99+a4447LK1/5yixfvjybNm1KkixcuDAnnnhiRo0alVmzZuVr\nX/ta+vr6sm3btsydOze/+MUvSv4IAEPGMD3AENvdnNGTTz55YK5oR0dHXve612XTpk35xCc+kac8\n5Sk59thj8z/+x//I2972tvT19eXZz352LrzwwoHn3XvvvZk1a1ZGjx6ds846K9OnT88999zT9J8N\nYKhV+vv7+0s3AuBgcO+99+ZVr3pV7r777tJNARg2DNMDAFCMMAoAQDGG6QEAKEbPKAAAxQijAAAU\nI4wCAFCMMAoAQDHCKAAAxQijAAAUI4wCAFDM/w/hQMzW8hF4EAAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7fe7c00df240>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"i8w-h8hVmMIJ","colab_type":"text"},"cell_type":"markdown","source":["> b) Construya y entrene una red neuronal profunda para clasificar las imágenes de MNIST utilizando la arquitectura propuesta en (a) y pre-entrenando los pesos de cada capa mediante un autoencoder básico. Proceda en modo clásico, es decir, entrenando en modo no supervisado una capa a la vez y tomando como input de cada nivel la representación (entrenada) obtenida en el nivel anterior. Después del entrenamiento efectúe un entrenamiento supervisado convencional (*fine-tunning*). **Compare los resultados de clasificación sobre el conjunto de pruebas con aquellos obtenidos en (a), sin pre-entrenamiento. Evalúe también los resultados antes del *fine-tunning*.** Comente.\n"]},{"metadata":{"id":"zq6ROzSKmUso","colab_type":"code","outputId":"39c0efbb-2790-4d5f-a50e-157512993ecf","executionInfo":{"status":"error","timestamp":1545663144027,"user_tz":180,"elapsed":3771,"user":{"displayName":"jose pablo fuenzalida","photoUrl":"","userId":"04170406534571537556"}},"colab":{"base_uri":"https://localhost:8080/","height":253}},"cell_type":"code","source":["from keras.datasets import mnist\n","... ## Load and preprocess MNIST as usual  ??\n","\n","autoencoder1 = load_model(DATA_PATH+'T3_P2_4_a.h5')\n","model = Sequential()\n","model.add(Dense(1000, activation='relu', input_shape=(784,)))\n","model.add(Dense(1000, activation='relu'))\n","model.add(Dense(10, activation='sigmoid'))\n","model.summary()\n","optimizer_ = SGD(lr=0.01, momentum=0.9)\n","model.compile(optimizer=SGD(lr=1.0), loss='mean_squared_error', metrics=['accuracy'])\n","history1 = model.fit(x_train, y_train_categ, nb_epoch=50, batch_size=25,shuffle=True, validation_data=(x_val, y_val_categ))\n","\n","autoencoder1.save(DATA_PATH+'P24_autoencoder_layer1.h5')\n","encoder1.save(DATA_PATH+'P24_encoder_layer1.h5')\n","\n","...###AUTOENCODER 2\n","x_train_encoded1 = encoder1.predict(x_train) #FORWARD PASS DATA THROUGH FIRST ENCODER\n","x_val_encoded1 = encoder1.predict(x_val)\n","x_test_encoded1 = encoder1.predict(x_test)\n","input_img2 = Input(shape=(n_hidden_layer1,))\n","encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n","decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n","autoencoder2 = Model(inputs=input_img2, outputs=decoded2)\n","encoder2 = Model(inputs=input_img2, outputs=encoded2)\n","autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n","history1 = autoencoder2.fit(x_train_encoded1,x_train_encoded1,nb_epoch=epochs_,batch_size=batch_size_,shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n","encoded_input2 = Input(shape=(n_hidden_layer2,))\n","autoencoder2.save(DATA_PATH+'P24_autoencoder_layer2.h5')\n","encoder2.save(DATA_PATH+'P24_encoder_layer2.h5')\n","...#FINE TUNNING\n","\n","from keras.models import Sequential\n","model = Sequential()\n","model.add(Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)))\n","model.layers[-1].set_weights(autoencoder1.layers[1].get_weights())\n","model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n","model.layers[-1].set_weights(autoencoder2.layers[1].get_weights())\n","model.add(Dense(10, activation='softmax'))\n","model.summary()\n","model.compile(optimizer=optimizer_,loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(x_train, Y_train,nb_epoch=20, batch_size=25,\n","shuffle=True, validation_data=(x_val, Y_val))\n","model.save('Net-768x1000x1000x10-finetunned.h5')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-59eb7e5af078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## Load and preprocess MNIST as usual  ??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mautoencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'T3_P2_4_a.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"]}]},{"metadata":{"id":"R0go6FkkmMNb","colab_type":"text"},"cell_type":"markdown","source":["> c) Repita usando funciones de **activación *tanh*. Comente**"]},{"metadata":{"id":"b8wP3g1zHJMX","colab_type":"text"},"cell_type":"markdown","source":["<a id=\"refs\"></a>\n","## Referencias\n","[1] George Kingsley Zipf (1949), *Human behavior and the principle of least effort*, Addison-Wesley Press  \n","[2] https://www.nvidia.es/object/cuda-parallel-computing-es.html  \n","[3] http://yann.lecun.com/exdb/mnist/  \n","[4] Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008, July). *Extracting and composing robust features with denoising autoencoders*. ACM."]},{"metadata":{"id":"ZwIEtcuu-oV_","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"Yv0rGkbJ-ptj","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"oGhLWtJd-mxQ","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"kztXZaES-lTe","colab_type":"text"},"cell_type":"markdown","source":[""]}]}